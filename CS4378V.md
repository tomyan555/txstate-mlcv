# CS4378V Introduction to Machine Learning, 2018 Spring
Instructor: [Dr. Yan (Tom) Yan] [[email](mailto:tom_yan@txstate.edu)]

Time: Tuesday and Thursday 03:30pm - 04:50pm

Location: UAC **00310**  

Office Hours: 
Thursday 01:00pm-03:00pm
COMAL Building 307G

## Course Description
Machine Learning is concerned with computer programs that automatically improve their performance through experience (e.g., that learn to recognize object in images or videos, recognize speech, classify text documents, detect credit card fraud, or drive autonomous robots). This course provides an in-depth understanding of machine learning and their applications in computer vision, multimedia and other domains.

Topics: probability distributions, regression, classification, kernel methods, clustering, semi-supervised learning, mixture models, graphical models, dimensionality reduction, manifold learning, sparse learning, multi-task learning, and transfer learning.

Homework assignments include both theoretic derivation and hands-on experiments with various learning algorithms. Every student is required to finish a project that is either assigned by the intructor or designed by the student himself/herself.

## Textbook
There is no required text for this course. Notes will be posted periodically on the Tracs.
The following books are recommended as optional reading:

* Pattern Recognition and Machine Learning, Christopher M. Bishop, 2006. [Webpage](http://research.microsoft.com/en-us/um/people/cmbishop/PRML/)

* The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Second Edition)
by Trevor Hastie, Robert Tibshirani and Jerome Friedman (2009) [Book](http://www-stat.stanford.edu/~hastie/Papers/ESLII.pdf)

## Course Announcements
Announcements will be emailed to the course mailing list. A welcome note will be sent to the mailing list at the beginning of the semester. If you do not receive the welcome message before the first class, please [send mail to me](mailto:tom_yan@txstate.edu).

## Policies
### Course Requirements and Grading
The grade will be calculated as follows:

* Assignments: 35%
* Project: 35% 
* Exam: 30% 
* Class participation: 5% 

## Prerequisite
Students are expected to have the following background:
* Knowledge of basic computer science principles and skills, at a level sufficient to
write a reasonably non-trivial computer program. 
* CS 3358 Data Structures
* Familiarity with the basic probability theory and the basic linear algebra 

### Homework
*Lateness and Extensions*

Homework is worth full credit at the beginning of class on the due date (later if an extension has been granted). It is worth at most 90% credit for the next 24 hours. It is worth at most 50% credit for the following 24 hours. It is worth 25% credit after that.  If you need an extension, please ask for it (by sending email to the instructor) as soon as the need for it is known.  Extensions that are requested promptly will be granted more liberally.  You must turn in all assignments.

*Collaboration among Students*

The purpose of student collaboration is to facilitate learning, not to circumvent it. Studying the material in groups is strongly encouraged. It is also allowed to seek help from other students in understanding the material needed to solve a particular homework problem, provided no written notes are shared, or are taken at that time, and provided learning is facilitated, not circumvented. The actual solution must be done by each student alone, and the student should be ready to reproduce their solution upon request. Any form of help or collaboration must be disclosed in full by all involved on the first page of their assignment. In any case, you must exercise academic integrity.

### Class Participation
Students are required to attend all classes and actively participate in discussions.

## Tentative Schedule

* Topic 1. Introduction 
* Topic 2. Basics - Probability Theory 
* Topic 3. Basics - Linear Algebra 
* Topic 4. Basics - Linear Algebra - SVD 
* Topic 5. Linear Models for Regression 
* Topic 6. Linear Models for Classification
* Topic 7. Support Vector Machines
* Topic 8. Kernel Methods
* Topic 9. Ensemble
* Topic 10. Tree Methods
* Topic 11. Clustering/Mixture Models
* Topic 12. Deep Learning
* Topic 13. Dimensionality Reduction
* Topic 14. Graphical Model 
* Topic 15. Sparse Learning 
* Topic 16. Matrix Completion and Collaborative Filtering
* Topic 17. Transfer and Multi-Task Learning 

## Other Policies

### Disabilities/Special Needs
If you are a student with a disability who will require an accommodation(s) to participate
in this course, please contact me within the first two weeks of the semester. You will be
asked to provide documentation from the Office of Disability Services. Failure to contact
me in a timely manner may delay your accommodations.

### Academic Integrity
You may discuss rough ideas and thoughts about homework and project with your other
classmates, but you have to write up your solution on your own. You are not allowed to
read, copy, or rewrite the solutions written by others (in this or previous terms or from
other sources such as internet). For group project, make sure to indicate the contribution
of each person. If you're stuck on a problem and unable to get to the instructor for help,
then we suggest you try and use hints from a publicly available source such as a textbook
or journal article. The source should be cited and you have to write the solution in your
own words. It should be apparent to us that you understand the solution for full credit.
The punishment for cheating on an assignment will be the docking of the final grade by
one mark (so, a C instead of a B for example). If two people are caught sharing solutions
then both the copier and copiee will be held equally responsible. Cheating on an exam
will result in failing the course.

See Student handbook for more information about Texas State Academic Policies
including probation, suspension, academic honesty, dropping a class, incompletes, grade
change, and withdrawal.


## References

### Linear Algebra and Matrix Computation 

* Gradient computation w.r.t. a vector/matrix

https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf

### Basic Probability Theory

* Shorter materials

http://ai.stanford.edu/~paskin/gm-short-course/lec1.pdf

http://www.sci.utah.edu/~gerig/CS6640-F2010/prob-tut.pdf 

* Longer books

http://mplab.ucsd.edu/tutorials/ProbabilityAndStats.pdf

https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf

### Basic Optimization 

* Lecture notes from Andrew Ng:

http://cs229.stanford.edu/notes/cs229-notes1.pdf

* If you are interested in systemtically studying the optimization 
 knowledge, try reading the book Convex Optimization:

https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf

The basic gradient descent is decribed in Page 463 (Page 477 of the PDF file). 
