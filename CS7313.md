# CS7313 - Advanced Machine Learning and Pattern Recognition, Spring 2018
**Instructor**: Yan (Tom) Yan [[email](mailto:tom_yan@txstate.edu)]

**Time**: Tuesday 06:30pm - 09:20pm

**Location**: CMAL 00103 

**Office Hours**: 
After the class / Tuesday and Thursday 01:00pm-03:30pm, COMAL Building 307G


## Course Description
Machine Learning is concerned with computer programs that automatically improve their performance through experience (e.g., that learn to recognize object in images or videos, recognize speech, classify text documents, detect credit card fraud, or drive autonomous robots). This course provides an in-depth understanding of machine learning and their applications in computer vision, multimedia and other domains.

Topics: regression, classification, kernel methods, clustering, semi-supervised learning, mixture models, dimensionality reduction, sparse learning, multi-task learning, transfer learning and deep learning.

Homework assignments include both theoretic derivation and hands-on experiments with various learning algorithms. Every student is required to finish a project that is either assigned by the intructor or designed by the student himself/herself.

## Textbook
There is no required text for this course. Notes will be posted periodically on the Tracs.
The following books are recommended as optional reading:

* Pattern Recognition and Machine Learning, by Christopher M. Bishop, 2006.

* The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Second Edition), 
by Trevor Hastie, Robert Tibshirani and Jerome Friedman, 2009. [Book](http://www-stat.stanford.edu/~hastie/Papers/ESLII.pdf)

* Deep Learning, by Ian Goodfellow, Yoshua Bengio and Aaron Courville, 2016.

## Grading Policies
The grade will be calculated as follows:

* Assignments: 30%
* Project: 35% 
* Exam: 30% 
* Class participation: 5% 

## Prerequisite
Students are expected to have the following background:
* Knowledge of basic computer science principles and skills, at a level sufficient to
write a reasonably non-trivial computer program. 
* CS 3358 Data Structures
* Familiarity with the basic probability theory and the basic linear algebra 

## Homework
*Lateness and Extensions*

Homework is worth full credit at the beginning of class on the due date (later if an extension has been granted). It is worth at most 90% credit for the next 24 hours. It is worth at most 50% credit for the following 24 hours. It is worth 25% credit after that.  If you need an extension, please ask for it (by sending email to the instructor) as soon as the need for it is known.  Extensions that are requested promptly will be granted more liberally.  You must turn in all assignments.

*Collaboration among Students*

The purpose of student collaboration is to facilitate learning, not to circumvent it. Studying the material in groups is strongly encouraged. It is also allowed to seek help from other students in understanding the material needed to solve a particular homework problem, provided no written notes are shared, or are taken at that time, and provided learning is facilitated, not circumvented. The actual solution must be done by each student alone, and the student should be ready to reproduce their solution upon request. Any form of help or collaboration must be disclosed in full by all involved on the first page of their assignment. In any case, you must exercise academic integrity.

## Class Participation
Students are required to attend all classes and actively participate in discussions.

## Tentative Schedule

* Topic 1. Introduction 
* Topic 2. Linear Regression, Gradient Descent  
* Topic 3. Linear Regression with multiple variables   
* Topic 4. Logistic Regression 
* Topic 5. Regularization    
* Topic 6. Support Vector Machines
* Topic 7. Clustering/Mixture Models
* Topic 8. Dimensionality Reduction 
* Topic 9. Large-scale Machine Learning 
* Topic 10. Sparse Learning
* Topic 11. Transfer and Multi-Task Learning 
* Topic 12. Neural Network 
* Topic 13. Deep Learning - Convolutional Neural Network (CNN)  
* Topic 14. Deep Learning - Recurrent Neural Network (RNN)   
* Topic 15. Deep Learning - Long Short-term Memory (LSTM)   
* Topic 16. Deep Learning - Generative adversarial network (GAN)

## Other Policies

### Disabilities/Special Needs
If you are a student with a disability who will require an accommodation(s) to participate
in this course, please contact me within the first two weeks of the semester. You will be
asked to provide documentation from the Office of Disability Services. Failure to contact
me in a timely manner may delay your accommodations.

### Academic Integrity
You may discuss rough ideas and thoughts about homework and project with your other
classmates, but you have to write up your solution on your own. You are not allowed to
read, copy, or rewrite the solutions written by others (in this or previous terms or from
other sources such as internet). For group project, make sure to indicate the contribution
of each person. If you're stuck on a problem and unable to get to the instructor for help,
then we suggest you try and use hints from a publicly available source such as a textbook
or journal article. The source should be cited and you have to write the solution in your
own words. It should be apparent to us that you understand the solution for full credit.
The punishment for cheating on an assignment will be the docking of the final grade by
one mark (so, a C instead of a B for example). If two people are caught sharing solutions
then both the copier and copiee will be held equally responsible. Cheating on an exam
will result in failing the course.

See Student handbook for more information about Texas State Academic Policies
including probation, suspension, academic honesty, dropping a class, incompletes, grade
change, and withdrawal.


## References

### Linear Algebra and Matrix Computation 

* Gradient computation w.r.t. a vector/matrix

https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf

### Basic Optimization 

* Lecture notes from Andrew Ng:

http://cs229.stanford.edu/notes/cs229-notes1.pdf

* If you are interested in systemtically studying the optimization 
 knowledge, try reading the book Convex Optimization:

https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf

The basic gradient descent is decribed in Page 463 (Page 477 of the PDF file). 
